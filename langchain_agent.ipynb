{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cuesuzuki/Day1Clone/blob/master/langchain_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain==0.0.157 openai==0.27.2 google-search-results==2.4.2"
      ],
      "metadata": {
        "id": "XzIIq1-F8fhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESqp93OI764l"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"76a1c41a1470be8d8bdc06827b4a4c3deeca924564791081ae331c8c6297e73c\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用するモデルを定義\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "k1SNvck47_ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google 検索を行う機能を追加\n",
        "from langchain import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events.\",\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "bW6fgx-67_k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 単純に入力を受け取ってモデルに入れるプロンプト\n",
        "prompt = PromptTemplate(input_variables=[\"query\"], template=\"{query}\")\n",
        "\n",
        "# chainの定義\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# toolsに追加して、モデルに使って貰う\n",
        "tools.append(\n",
        "    Tool(\n",
        "        name=\"Language Model\",\n",
        "        func=llm_chain.run,\n",
        "        description=\"use this tool for general purposes like answer question and output formatting\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "7Dmpvd5L8Bzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 計算機能を追加\n",
        "from langchain.chains import LLMMathChain\n",
        "llm_math = LLMMathChain(llm=llm)\n",
        "\n",
        "tools.append(Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math.run,\n",
        "        description=\"Useful for when you need to answer questions about calculation.\",\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp_bcwVI8B6c",
        "outputId": "e04d8cfb-1de5-44c6-c979-9fbd15ea8016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agentの定義\n",
        "agent = initialize_agent(\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "AJyxk9rY8CAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.agent.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtLWbIzo9bAP",
        "outputId": "c6c30939-6691-4932-cb8c-09a1eaba2916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Search: useful for when you need to answer questions about current events.\n",
            "Language Model: use this tool for general purposes like answer question and output formatting\n",
            "Calculator: Useful for when you need to answer questions about calculation.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Search, Language Model, Calculator]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    response = agent.run(\"Tell us about Piketty's Capital in the 21st Century in 30 words or less.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrcQvekJ9bIl",
        "outputId": "229c42c6-5700-43a5-91da-7a5246c177a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to search for the answer to this question.\n",
            "Action: Search\n",
            "Action Input: \"2023 WBC winner country\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJapan won their record-extending third title after defeating defending champions United States 3–2 in the championship game, becoming the second team since the ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found the answer to the question.\n",
            "Final Answer: Japan won the 2023 WBC.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PM13jLhU9ihW",
        "outputId": "4d3df847-7aaf-4d54-e7d7-2ff00399c1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023年WBCの優勝国は日本代表野球チームでした。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3lwmL1Q9kOT",
        "outputId": "05bfff45-2c99-415f-e69d-3a75dfe12669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokens Used: 1672\n",
              "\tPrompt Tokens: 1510\n",
              "\tCompletion Tokens: 162\n",
              "Successful Requests: 6\n",
              "Total Cost (USD): $0.0033439999999999998"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb.total_cost * 136"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "genpjbk_9mZF",
        "outputId": "0fe103e7-bf57-43fa-f974-d42e791916b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45478399999999997"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. LLMｓの使い方"
      ],
      "metadata": {
        "id": "zr3hj1jE-gxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import openai\n",
        "\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "llm(\"Tell me about Piketty's 'Capital in the 21st Century' in 30 words or less.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "yh1d3zZ7-gWs",
        "outputId": "ea8f949e-9ad3-4cc0-9d50-b92ff57464da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3a5f45fc93be>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tell me about Piketty's 'Capital in the 21st Century' in 30 words or less.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     ) -> BaseMessage:\n\u001b[0;32m--> 175\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         ).generations[0][0]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmessage_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         run_manager = callback_manager.on_llm_start(\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmessage_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         run_manager = callback_manager.on_llm_start(\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema.py\u001b[0m in \u001b[0;36mget_buffer_string\u001b[0;34m(messages, human_prefix, ai_prefix)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got unsupported message type: {m}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mstring_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{role}: {m.content}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Got unsupported message type: T"
          ]
        }
      ]
    }
  ]
}